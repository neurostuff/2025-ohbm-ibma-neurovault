{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bed8aed8",
   "metadata": {},
   "source": [
    "# OHBM 2025 IBMA with NeuroVault tutorial\n",
    "\n",
    "## IBMA: Image-based meta-analysis\n",
    "\n",
    "IBMA, the gold standard of neuroimaging meta-analysis (Salimi-Khorshidi et al., 2009; Salo et al., 2023), consists of aggregating results from group-level, whole-brain statistical maps from individually conducted functional magnetic resonance imaging (fMRI) studies. IBMA outperforms other popular meta-analysis methods, such as coordinate-based meta-analysis (CBMA). IBMA methods use whole-brain statistics; thus, all existing voxel-wise statistical methods are available to analyze subject-level data within studies (Lazar et al., 2002). IBMA produces richer and more detailed results, with greater sensitivity to detect brain structures that are often absent from CBMA results. IBMA also has greater power; thus, one could potentially achieve similar or even better results with a small fraction of studies generally required in CBMA. In addition, when both the parameter (i.e., activation) and variance estimates are available, hierarchical mixed (random) effect models can be used to account for both within- and between-study variance (Salimi-Khorshidi et al., 2009).\n",
    "\n",
    "## Tools\n",
    "\n",
    "### NiMARE\n",
    "\n",
    "![NiMARE banner](images/nimare_banner.png)\n",
    "\n",
    "[NiMARE](https://nimare.readthedocs.io/en/latest/) is a Python library for performing neuroimaging meta-analyses and related analyses, like automated annotation of academic texts and functional decoding. The goal of NiMARE is to centralize and standardize implementations of common meta-analytic tools, so that researchers can use whatever tool is most appropriate for a given research question.\n",
    "\n",
    "### NeuroVault\n",
    "\n",
    "![NeuroVault logo](images/neurovault-logo.svg)\n",
    "\n",
    "[NeuroVault](https://neurovault.org) is a web-based repository of fMRI statistical maps from neuroimaging studies (Gorgolewski et al., 2015). The brain maps are grouped in collections that are created and updated voluntarily. This repository can be explored and downloaded with the help of an API, which is supported by some Python neuroimaging tools (e.g., Nilearn and NiMARE).\n",
    "\n",
    "### Cognitive Atlas\n",
    "\n",
    "![CogAt logo](images/cogat-logo.png)\n",
    "\n",
    "[Cognitive Atlas](https://www.cognitiveatlas.org/) (Poldrack et al., 2011) is an online repository of cumulative knowledge from experienced researchers from the psychology, cognitive science, and neuroscience fields. We will focus on two entities: 907 cognitive concepts and 841 tasks with definitions and properties. Cognitive concepts contain relationships with other concepts and tasks, to establish a map between mental processes and brain function. Cognitive Atlas provides an API to download the database, which is integrated into NiMARE.\n",
    "\n",
    "## Goals for this tutorial\n",
    "\n",
    "1. Download data from NeuroVault\n",
    "2. Identify usable Neurovault images for IBMA\n",
    "3. Learn to identify outliers\n",
    "4. Use NiMARE to run IBMA\n",
    "5. Interpret IBMA results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46d986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os.path as op\n",
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from nimare.reports.base import run_reports\n",
    "from nimare.workflows import IBMAWorkflow\n",
    "import requests\n",
    "from nimare.meta.ibma import Stouffers\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn.plotting import plot_stat_map\n",
    "from IPython.display import HTML\n",
    "\n",
    "from utils import download_images, convert_to_nimare_dataset\n",
    "from outliers import remove_outliers\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger(\"nimare\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e5f345",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = op.abspath(\"../data\")\n",
    "notebook_dir = op.abspath(\".\")\n",
    "filename = \"november_2022\"\n",
    "nv_data_dir = op.join(data_dir, filename)\n",
    "image_dir = op.join(data_dir, \"nv_images\")\n",
    "report_dir = op.join(data_dir, \"report\")\n",
    "report_clean_dir = op.join(data_dir, \"report_clean\")\n",
    "\n",
    "os.makedirs(data_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66ad6c1",
   "metadata": {},
   "source": [
    "Download Neurovault data\n",
    "\n",
    "To explore the NeuroVault database, we created an SQL query and exported the database contents to human-readable tables while filtering sensitive user information. This provided sufficient metadata from all collections and images to investigate the entire database without downloading the files. The images identified as usable for IBMA (see the following section on the image selection framework) were downloaded along with their metadata and converted to a NiMARE Dataset object to leverage existing IBMA methods implemented in NiMARE.\n",
    "\n",
    "**Note**: The following three cells, which download the NeuroVault data tables, need to be run only once in this tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48caca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "github_url = (\n",
    "    f\"https://raw.githubusercontent.com/NeuroVault/nv-data/master/{filename}.tar.gz\"\n",
    ")\n",
    "response = requests.get(github_url)\n",
    "\n",
    "file_path = op.join(data_dir, f\"{filename}.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135306ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, \"wb\") as f:\n",
    "    f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a301fa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tarfile.open(file_path, \"r:gz\") as tar:\n",
    "    tar.extractall(path=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21570bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(os.listdir(nv_data_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5f0131",
   "metadata": {},
   "source": [
    "The files available are the following:\n",
    "\n",
    "- `django_content_type.csv`: Not relevant\n",
    "- `statmaps_atlas.csv`: Contains atlases available in NeuroVaults\n",
    "- `statmaps_basecollectionitem.csv`: Contains the image IDs, name, description, and the associated collection ID\n",
    "- `statmaps_cognitiveatlascontrast.csv`: Contains the contrast names and IDs from Cognitive Atlas\n",
    "- `statmaps_cognitiveatlastask.csv`: Contains the task names and IDs from Cognitive Atlas\n",
    "- `statmaps_collection.csv`: Contains the collection-level metadata, including name, DOI, authors, etc.\n",
    "- `statmaps_collection_communities.csv`: Contains the communities linked to a collection\n",
    "- `statmaps_collection_contributors.csv`: Contains the user ID linked to a collection\n",
    "- `statmaps_community.csv`: Contains the information about communities in NeuroVault, such as their label and description\n",
    "- `statmaps_image.csv`: Contains image file name and demographic information linked to the image\n",
    "- `statmaps_statisticmap.csv`: Contains image metadata such as map type (e.g., Z), modality (e.g., fMRI), task (e.g., stroop), analysis level (e.g., single subject versus group versus meta-analysis), etc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4efe198",
   "metadata": {},
   "source": [
    "To identify images usable for IBMA, we first retrieve the image's ID (`id`) and name (`name`), as well as their affiliated collection ID (`collection_id`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8b242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get images and collection IDs\n",
    "cmeta_cols = [\"id\", \"collection_id\", \"name\"]\n",
    "image_collection = pd.read_csv(\n",
    "    op.join(nv_data_dir, \"statmaps_basecollectionitem.csv\"),\n",
    "    usecols=cmeta_cols,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20664c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get images and collection IDs\n",
    "cmeta_cols2 = [\"id\", \"DOI\"]\n",
    "coll_meta = pd.read_csv(\n",
    "    op.join(nv_data_dir, \"statmaps_collection.csv\"),\n",
    "    usecols=cmeta_cols2,\n",
    ")\n",
    "coll_meta = coll_meta.rename(columns={\"id\": \"collection_id\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c19ab8",
   "metadata": {},
   "source": [
    "Second, from `statmaps_image.csv` we get the file name, which will use later to download the images using the NeuroVault API. In this database `basecollectionitem_ptr_id` refers to the image ID `id`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc35780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get path to images in NeuroVault\n",
    "image_cols = [\"file\", \"basecollectionitem_ptr_id\"]\n",
    "image = pd.read_csv(\n",
    "    op.join(nv_data_dir, \"statmaps_image.csv\"),\n",
    "    usecols=image_cols,\n",
    ")\n",
    "image = image.rename(columns={\"basecollectionitem_ptr_id\": \"id\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc6913c",
   "metadata": {},
   "source": [
    "Third, some relevant images and metadata are extracted from `statmaps_statisticmap.csv`.\n",
    "\n",
    "The selected metadata included:\n",
    "\n",
    "- `image_ptr_id`: Image ID in NeuroVault\n",
    "\n",
    "- `map_type`: Type of statistic that is the basis of the inference\n",
    "\n",
    "  - Possible cases: `T map`, `Z map`, `F map`, `Chi squared map`, `P map (given null hypothesis)`, `1-P map (\"inverted\" probability)`, `multivariate-beta map`, `Beta map`, `ROI/mask`, `parcellation`, `anatomical`, `variance`, `Other`\n",
    "\n",
    "- `modality`: Brain imaging procedure that was used to acquire the data\n",
    "\n",
    "  - Possible cases: `fMRI-BOLD`, `fMRI-CBF`, `fMRI-CBV`, `Diffusion MRI`, `Structural MRI`, `PET FDG`, `PET [150]-water`, `MEG`, `EEG`, `Other`\n",
    "\n",
    "- `analysis_level`: What level of summary data was used as the input to this analysis?\n",
    "\n",
    "  - Possible cases: `single-subject`, `group`, `meta-analysis`, `other`\n",
    "\n",
    "- `number_of_subjects`: Number of subjects used to generate this map\n",
    "\n",
    "  - Integer values ranging from 1 to N\n",
    "\n",
    "- `is_thresholded`: Whether the map is thresholded or not\n",
    "\n",
    "  - Possible cases: `True`, `False`\n",
    "\n",
    "- `brain_coverage`: Percentage of brain coverage\n",
    "\n",
    "- `not_mni`: Whether the image is in the MNI space or not\n",
    "\n",
    "  - Possible cases: `True`, `False`\n",
    "\n",
    "- `cognitive_paradigm_cogatlas_id`: Task performed by the subjects in the scanner described using [Cognitive Atlas](https://www.cognitiveatlas.org/)\n",
    "\n",
    "  **Note**: One can find the complete list of Cognitive Atlas tasks and their IDs in the file `statmaps_cognitiveatlastask.csv`, or in the Cognitive Atlas [website](https://www.cognitiveatlas.org/tasks), which are also accessible via [API](https://www.cognitiveatlas.org/api/v-alpha/task).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57d35fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get image metadata\n",
    "imeta_cols = [\n",
    "    \"image_ptr_id\",\n",
    "    \"map_type\",\n",
    "    \"modality\",\n",
    "    \"analysis_level\",\n",
    "    \"number_of_subjects\",\n",
    "    \"is_thresholded\",\n",
    "    \"brain_coverage\",\n",
    "    \"not_mni\",\n",
    "    \"cognitive_paradigm_cogatlas_id\",\n",
    "]\n",
    "image_meta = pd.read_csv(\n",
    "    op.join(nv_data_dir, \"statmaps_statisticmap.csv\"),\n",
    "    usecols=imeta_cols,\n",
    ")\n",
    "image_meta = image_meta.rename(columns={\"image_ptr_id\": \"id\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a4f773",
   "metadata": {},
   "source": [
    "Finally, we combine the previously extracted information into a single Pandas DataFrame by merging on the image ID (`id`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c524b66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_merge = pd.merge(image_collection, image, how=\"left\", on=\"id\")\n",
    "image_merge = pd.merge(image_merge, coll_meta, how=\"left\", on=\"collection_id\")\n",
    "image_full_df = pd.merge(image_merge, image_meta, how=\"left\", on=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fb5492",
   "metadata": {},
   "source": [
    "# Select images for IBMA\n",
    "\n",
    "Using the available metadata from the retrieved tables, we set different inclusion criteria for images to be considered for a meta-analysis. We focused on fMRI-BOLD images, as they are the most prevalent modality in NeuroVault. Note that the methods presented in this tutorial should work with other image modalities (e.g., PET, diffusion MRI, structural MRI). Still, only fMRI-BOLD had enough data in NeuroVault for meta-analyses. Then, we specifically chose images from group-level analyses. Additionally, we retained only images from studies with a sample size greater than ten subjects. Next, we selected images classified as T or Z statistics. Although best practices in meta-analysis suggest using meaningful units and incorporating uncertainty through standard errors, T/Z statistic maps are the most commonly shared images in NeuroVault (Maumet and Nichols, 2016). We discuss this further in the following sections. Following that, we retained unthresholded images that cover 40% of the brain and are in MNI space. Ultimately, we narrowed our selection to images associated with a Cognitive Atlas task.\n",
    "\n",
    "For this tutorial, we use motor domain including the motor fMRI task paradigm (`trm_550b53d7dd674`), motor sequencing task (`tsk_4a57abb949bbf`), and finger tapping task (`trm_4c898f079d05e`) as it has been validated before (Peraza et al., 2025). Users are encouraged to investigate other tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b312dcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_selected_df = image_full_df.query(\n",
    "    'modality == \"fMRI-BOLD\"'\n",
    "    ' & analysis_level == \"G\"'\n",
    "    ' & number_of_subjects > 10'\n",
    "    ' & (map_type == \"Z\" | map_type == \"T\")'\n",
    "    ' & is_thresholded == \"f\"'\n",
    "    ' & brain_coverage > 40'\n",
    "    ' & not_mni == \"f\"'\n",
    "    ' & (cognitive_paradigm_cogatlas_id == \"trm_550b53d7dd674\"'\n",
    "    ' | cognitive_paradigm_cogatlas_id == \"tsk_4a57abb949bbf\"'\n",
    "    ' | cognitive_paradigm_cogatlas_id == \"trm_4c898f079d05e\")' \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3727f4a8",
   "metadata": {},
   "source": [
    "# Download images for IBMA\n",
    "\n",
    "Next, we attempt to download the selected image from NeuroVault. To facilitate that process, we implemented a function named download images, which takes the image IDs from the previously created Python DataFrame and a path to a directory to download the images. Note that not all images can be downloaded; some of them belong to a private collection, while others have a corrupted file.\n",
    "\n",
    "Given that some images will have the same image name (e.g., z*stats.nii.gz), we downloaded the images, adding a unique identifiable prefix composed of the collection ID and the image ID. The filename for the image follows this pattern: , `[col_id]-[img_id]*[image_name].nii.gz`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e32eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduced dataset for tutorial only, use image_selected_df otherwise\n",
    "image_selected_reduced = image_selected_df.sample(\n",
    "    n=20, random_state=42, replace=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0d9d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = image_selected_reduced[\"id\"].unique()\n",
    "image_usable_df = download_images(image_ids, image_dir)\n",
    "image_df = pd.merge(image_selected_reduced, image_usable_df, on=\"id\")\n",
    "\n",
    "print(f\"Usable images: {len(image_usable_df)}/{len(image_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9697c536",
   "metadata": {},
   "source": [
    "# Create NiMARE Dataset object for IBMA\n",
    "\n",
    "Then, we create a NiMARE Dataset object in order to leverage image-based meta-analytic methods from NiMARE. In NiMARE, datasets are stored in a special `Dataset` class. The `Dataset` class stores the most relevant information as properties.\n",
    "\n",
    "For users' convenience, we have implemented a function `convert_to_nimare_dataset` to facilitate that process. The function takes a Pandas DataFrame and returns a NiMARE Database object.\n",
    "\n",
    "In addition, we use the `ImageTransformer`, which can generate images from other images, as long as the right images and metadata are available. In this case, we generate z-statistic images from t-statistic maps, leveraging the sample size information in the metadata.\n",
    "\n",
    "For additional detail on creating a Dataset class, please refer to this example: https://nimare.readthedocs.io/en/stable/auto_examples/01_datasets/06_plot_dataset_json.html#sphx-glr-auto-examples-01-datasets-06-plot-dataset-json-py.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b586255",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_cols = [\"name\", \"file\"]\n",
    "dset = convert_to_nimare_dataset(\n",
    "    image_df,\n",
    "    study_col=\"collection_id\",\n",
    "    contrast_col=\"id\",\n",
    "    sample_size_col=\"number_of_subjects\",\n",
    "    map_type_col=\"map_type\",\n",
    "    path_col=\"path\",\n",
    "    metadata_cols=metadata_cols,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fd815a",
   "metadata": {},
   "source": [
    "The full list of identifiers in the Dataset is located in `Dataset.ids`. Identifiers are composed of two parts- a study ID and a contrast ID. Within the Dataset, those two parts are separated with a `-`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca396717",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dset.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c920d20b",
   "metadata": {},
   "source": [
    "Most other information is stored in `pandas` DataFrames. The five DataFrame-based attributes are `Dataset.metadata`, `Dataset.coordinates`, `Dataset.images`, `Dataset.annotations`, and `Dataset.texts`.\n",
    "\n",
    "Each DataFrame contains at least three columns: `study_id`, `contrast_id`, and `id`, which is the combined `study_id` and `contrast_id`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff36e34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset.metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982df668",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset.images.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0c1774",
   "metadata": {},
   "source": [
    "# Initialize an IBMA workflow with NiMARE\n",
    "\n",
    "Next, we leverage the available NiMARE workflows that implement several meta-analytic pipelines. Specifically, we will use an IBMAWorkflow\n",
    "\n",
    "The parameters to initialize the workflow are:\n",
    "\n",
    "- `estimator`: Meta-analysis estimator. Default Stouffers\n",
    "\n",
    "  Useful parameter in Stouffers:\n",
    "\n",
    "  - `aggressive_mask`: By default, all image-based meta-analysis estimators adopt an aggressive masking strategy, in which any voxels with a value of zero in any of the input maps will be removed from the analysis. Setting `aggressive_mask=False` will instead run the analysis for all voxels that have a value from at least two study level statistical maps\n",
    "\n",
    "  - `use_sample_size`: Whether to use sample sizes for weights (i.e., \"weighted Stouffer's\") or not.\n",
    "\n",
    "  - `normalize_contrast_weights`: Whether to use a number of contrasts per study to normalize the weights or not.\n",
    "\n",
    "  **Note**: The current implementation in NiMARE adds a correction factor to account for repeated scans (i.e., multiple samples from the same study).\n",
    "\n",
    "- `corrector`: Meta-analysis corrector. Default FDR\n",
    "\n",
    "- `diagnostics`: Diagnostic method. Default Jackknife\n",
    "\n",
    "- `voxel_thresh`: An optional voxel-level threshold that may be applied to the `target_image` of the diagnostic class to define clusters. Default 1.6\n",
    "\n",
    "- `cluster_threshold`: Cluster size threshold. Default 10\n",
    "\n",
    "- `output_dir`: Output directory in which to save results. Default None\n",
    "\n",
    "- `n_cores`: Number of cores to use for parallelization. Default 1\n",
    "\n",
    "  Here, parallelization only applies to the Jackknife analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102900e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = IBMAWorkflow(\n",
    "    estimator=Stouffers(\n",
    "        aggressive_mask=True,\n",
    "        use_sample_size=True,\n",
    "        normalize_contrast_weights=True,\n",
    "    ),\n",
    "    corrector=\"fdr\",\n",
    "    diagnostics=\"jackknife\",\n",
    "    voxel_thresh=2.6,\n",
    "    cluster_threshold=90,\n",
    "    output_dir=None,\n",
    "    n_cores=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53af63d6",
   "metadata": {},
   "source": [
    "# Apply workflow to the Dataset object\n",
    "\n",
    "The fit method of an IBMA workflow class runs the following steps:\n",
    "\n",
    "1. Runs a meta-analysis using the specified method (default: Stouffers)\n",
    "\n",
    "2. Applies a corrector to the meta-analysis results (default: FDRCorrector, indep)\n",
    "\n",
    "3. Generates cluster tables and runs diagnostics on the corrected results (default: Jackknife)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8edea1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = workflow.fit(dset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661639c3",
   "metadata": {},
   "source": [
    "# Plot Results\n",
    "\n",
    "The fit method of the IBMA workflow class returns a MetaResult object, where you can access the corrected results of the meta-analysis and diagnostics tables.\n",
    "\n",
    "Corrected map:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b959b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = results.get_map(\"z_corr-FDR_method-indep\")\n",
    "plot_stat_map(img, threshold=3.29)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50e029f",
   "metadata": {},
   "source": [
    "## Clusters table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bf521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.tables[\"z_corr-FDR_method-indep_tab-clust\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707e2f50",
   "metadata": {},
   "source": [
    "## Diagnostic: Jackknife\n",
    "\n",
    "The Jackknife analysis characterizes the relative contribution of each experiment in a meta-analysis to the resulting clusters by looping through experiments, calculating the Estimator's summary statistic for all experiments except the target experiment, dividing the resulting test summary statistics by the summary statistics from the original meta-analysis, and finally averaging the resulting proportion values across all voxels in each cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d37ca55",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.tables[\"z_corr-FDR_method-indep_diag-Jackknife_tab-counts\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da05a274",
   "metadata": {},
   "source": [
    "# Generate an HTML report of IBMA results\n",
    "\n",
    "Finally, a NiMARE report is generated from the MetaResult. The `run_reports` function takes two arguments: (1) A [MetaResult](https://nimare.readthedocs.io/en/stable/generated/nimare.results.MetaResult.html#nimare.results.MetaResult) object produced by the IBMA workflow, and (2) a path to an output directory in which to save the report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5bb5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_reports(results, report_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461b3eb5",
   "metadata": {},
   "source": [
    "# Exclude spurious images\n",
    "\n",
    "## Identify and exclude non-statistical maps\n",
    "\n",
    "Even after applying the previous strict preliminary inclusion criteria, we still found plenty of incorrectly annotated images, especially representing other image modalities and others with extreme values. Therefore, we developed an automatic heuristic selection to remove those spurious images from the meta-analysis. Images with a minimum Z value (`zmin`) smaller than 1.96 (i.e., Z score for a 0.05 p-value) were removed as they potentially consisted of mislabeled correlation maps, inverted p-value maps, or did not contain statistically significant voxels. We also excluded images with a maximum Z score (`zmax`) larger than 20. Although the number 20 is arbitrary, we wanted to detect images with an unusually large signal. For example, mislabeled BOLD or COPE (contrast of parameter estimates) images or others resulting from studies with a huge sample size. Additionally, using the image metadata, we analyzed the image and file name. We removed those containing keywords such as \"ICA,\" \"PCA,\" \"PPI,\" \"seed,\" \"functional connectivity,\" “cope,” “tfce,” and \"correlation,\" which represent modalities not of interest for the meta-analysis of the current work. Example: https://neurovault.org/images/31938/\n",
    "\n",
    "## Identify and exclude duplicate and inverted contrast images\n",
    "\n",
    "It is quite common for NeuroVault users to upload inverted contrasts and duplicates. For example, one might find two images representing the same contrast (such as House > Face) but with the signs reversed (i.e., Face > House). Example of inverted contrasts in NeuroVault: [MAINFOOD_HCplus>HCmin](https://neurovault.org/images/123498/) and [MAINFOOD_HCmin>HCplus](https://neurovault.org/images/123499/). This creates problems for meta-analyses, as these images effectively cancel each other out when aggregated.\n",
    "\n",
    "Additionally, it is typical for users to upload multiple images of the same contrast, differing only by the covariate used in the group-level analysis. These can be considered duplicates, especially when the covariate does not influence the final estimate. To identify duplicates, we utilize the correlation matrix of the input samples. Image pairs with a correlation close to 1 are considered duplicates, while those with a correlation close to -1 are labeled as inverted contrasts. From the identified duplicates, we randomly selected one image from each pair. For pairs of inverted contrasts, we choose the image with the more positive correlation to the median image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e302555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_clean = remove_outliers(\n",
    "    dset,\n",
    "    zmin=1.96,\n",
    "    zmax=15,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f9e092",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_clean.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9562b3",
   "metadata": {},
   "source": [
    "Run the IBMA workflow on the clean dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a805264",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_clean = IBMAWorkflow(\n",
    "    estimator=Stouffers(\n",
    "        aggressive_mask=True,\n",
    "        use_sample_size=True,\n",
    "        normalize_contrast_weights=True,\n",
    "    ),\n",
    "    corrector=\"fdr\",\n",
    "    diagnostics=\"jackknife\",\n",
    "    voxel_thresh=2.6,\n",
    "    cluster_threshold=90,\n",
    "    output_dir=None,\n",
    "    n_cores=1,\n",
    ")\n",
    "results_clean = workflow_clean.fit(dset_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ec241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_reports(results_clean, report_clean_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d4ce1a",
   "metadata": {},
   "source": [
    "# Caveats\n",
    "\n",
    "- **Assumption of independence:** We considered all images to be independent of each other. However, many NeuroVault images originate from the same paper or collection, and thus are not truly independent. This can lead to inflated false positive rates due to shared analytic pipelines or overlapping subject populations.\n",
    "\n",
    "- **Contrast heterogeneity:** The analysis may combine images representing different contrasts or experimental conditions. This heterogeneity can obscure or confound meta-analytic results. We recommend performing a manual review to select images corresponding to the specific contrast of interest for your research question.\n",
    "\n",
    "- **Metadata accuracy:** The inclusion criteria rely on metadata provided by NeuroVault contributors. Misannotations or incomplete metadata (e.g., incorrect map type, modality, or sample size) can result in inappropriate inclusion or exclusion of images.\n",
    "\n",
    "- **Residual spurious images:** Despite automated outlier and quality control steps, some spurious, mislabeled, or low-quality images may remain in the dataset, potentially biasing results.\n",
    "\n",
    "- **Sample size reporting:** Not all images have accurate or available sample size information, which may affect the weighting in meta-analytic estimators that use sample size.\n",
    "\n",
    "- **Generalizability:** The workflow is tailored to fMRI-BOLD group-level Z/T maps for three domains (i.e., working memory, motor, and emotion processing). Results and best practices may differ for other modalities, analysis levels, or cognitive domains.\n",
    "\n",
    "- **Publication bias:** Only a subset of studies share their statistical maps on NeuroVault, which may introduce publication or selection bias into the meta-analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cadb057",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "This tutorial demonstrated a complete workflow for performing image-based meta-analysis (IBMA) using open neuroimaging data from NeuroVault and the NiMARE Python library. We began by downloading and exploring NeuroVault metadata, then applied strict inclusion criteria to select appropriate group-level, unthresholded fMRI-BOLD statistical maps related to motor tasks. After downloading the usable images, we constructed a NiMARE Dataset and ran an IBMA workflow using Stouffer's method, including FDR correction and jackknife diagnostics. We also implemented additional quality control steps to remove spurious, duplicate, and inverted images, and repeated the meta-analysis on the cleaned dataset. The notebook provides a reproducible pipeline for robust IBMA, highlights best practices for data selection and cleaning, and generates both visual and tabular outputs to interpret the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7211d62b",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "Before performing your meta-analysis, carefully review the following considerations:\n",
    "\n",
    "- **Check for independence:** When selecting images, ensure that you do not include multiple images from the same collection or study unless justified, as this can violate the assumption of independence and inflate false positives.\n",
    "- **Refine contrast selection:** Manually inspect and select images representing the same experimental contrast or condition to reduce heterogeneity and improve interpretability.\n",
    "- **Verify metadata accuracy:** Cross-check metadata (e.g., map type, modality, sample size) with the original publications to avoid misannotations or incomplete information.\n",
    "- **Assess image quality:** Visually inspect statistical maps and exclude any that appear spurious, mislabeled, or of low quality, even after automated QC.\n",
    "- **Consider sample size reporting:** Be aware that missing or inaccurate sample size information can affect meta-analytic weighting; document any assumptions or imputations.\n",
    "- **Acknowledge generalizability:** The workflow was tested for group-level fMRI-BOLD Z/T maps in three domains (i.e., working memory, motor, and emotion processing); adapt criteria as needed for other modalities or domains.\n",
    "- **Be aware of publication bias:** Recognize that NeuroVault may not represent all studies, and results could be influenced by selective data sharing. \n",
    "\n",
    "By addressing these points, you can enhance the rigor and reproducibility of your IBMA.\n",
    "\n",
    "1. For your meta-analysis research, start by visiting the [Cognitive Atlas](https://www.cognitiveatlas.org/) database to identify your domain of interest. Choose specific tasks related to the selected area and evaluate the available data in NeuroVault.\n",
    "\n",
    "2. For the selected data, review the linked papers thoroughly to understand their methodologies and results. Ensure that you identify the contrasts of interest.\n",
    "\n",
    "3. This approach reflects the current best practices for IBMA using NeuroVault. Please note that this workflow will ultimately be integrated into [Neurosynth Compose](https://compose.neurosynth.org/) to streamline the meta-analysis process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87fdf66",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "For additional detail on the methods presented in this tutorial plase refer to our preprint: https://doi.org/10.1101/2025.03.06.641922\n",
    "\n",
    "- Gorgolewski, K.J., Varoquaux, G., Rivera, G., Schwarz, Y., Ghosh, S.S., Maumet, C., Sochat, V.V., Nichols, T.E., Poldrack, R.A., Poline, J.-B., Yarkoni, T., Margulies, D.S., 2015. NeuroVault.org: a web-based repository for collecting and sharing unthresholded statistical maps of the human brain. Frontiers in Neuroinformatics 9, 8. https://doi.org/10.3389/fninf.2015.00008\n",
    "\n",
    "- Lazar, N.A., Luna, B., Sweeney, J.A., Eddy, W.F., 2002. Combining Brains: A Survey of Methods for Statistical Pooling of Information. NeuroImage 16, 538–550. https://doi.org/10.1006/nimg.2002.1107\n",
    "\n",
    "- Maumet, C., Nichols, T.E., 2016. Minimal Data Needed for Valid & Accurate Image-Based fMRI Meta-Analysis. https://doi.org/10.1101/048249\n",
    "\n",
    "- Peraza, J.A., Kent, J.D., Blair, R.W., Poline, J.-B., Nichols, T.E., Vega, A. de la, Laird, A.R., 2025. Advancing image-based meta-analysis for fMRI: A framework for leveraging NeuroVault data. https://doi.org/10.1101/2025.03.06.641922\n",
    "\n",
    "- Poldrack, R.A., Kittur, A., Kalar, D., Miller, E., Seppa, C., Gil, Y., Parker, D.S., Sabb, F.W., Bilder, R.M., 2011. The cognitive atlas: toward a knowledge foundation for cognitive neuroscience. Front Neuroinform 5, 17. https://doi.org/10.3389/fninf.2011.00017\n",
    "\n",
    "- Salimi-Khorshidi, G., Smith, S.M., Keltner, J.R., Wager, T.D., Nichols, T.E., 2009. Meta-analysis of neuroimaging data: A comparison of image-based and coordinate-based pooling of studies. NeuroImage 45, 810–823. https://doi.org/10.1016/j.neuroimage.2008.12.039\n",
    "\n",
    "- Salo, T., Yarkoni, T., Nichols, T.E., Poline, J.-B., Bilgel, M., Bottenhorn, K.L., Jarecka, D., Kent, J.D., Kimbler, A., Nielson, D.M., Oudyk, K.M., Peraza, J.A., Pérez, A., Reeders, P.C., Yanes, J.A., Laird, A.R., 2023. NiMARE: Neuroimaging Meta-Analysis Research Environment. Aperture Neuro 3, 1–32. https://doi.org/10.52294/001c.87681\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4618fd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
